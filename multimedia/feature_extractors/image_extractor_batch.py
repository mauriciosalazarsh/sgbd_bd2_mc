# multimedia/feature_extractors/image_extractor_batch.py
import numpy as np
import pickle
import os
import gc
from typing import List, Tuple, Optional, Any, Union

# Importaciones seguras para evitar errores
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    cv2 = None

class BatchImageFeatureExtractor:
    def __init__(self, method='sift', batch_size=100):
        """
        Extractor de caracter√≠sticas optimizado para lotes
        
        Args:
            method: 'sift' √∫nicamente por ahora
            batch_size: tama√±o del lote para procesamiento
        """
        self.method = method.lower()
        self.batch_size = batch_size
        self.sift = None
        
        if self.method == 'sift':
            if not CV2_AVAILABLE or cv2 is None:
                raise ImportError("OpenCV no est√° instalado. Ejecuta: pip install opencv-python")
            
            # Crear detector SIFT con par√°metros optimizados
            self.sift = cv2.SIFT_create(
                nfeatures=128,  # Limitar n√∫mero de keypoints
                contrastThreshold=0.08,  # Aumentar threshold para menos keypoints
                edgeThreshold=15,  # Aumentar para filtrar m√°s
                sigma=1.2  # Reducir sigma para procesamiento m√°s r√°pido
            )
    
    def extract_sift_features_batch(self, image_paths: List[str], save_every: int = 1000) -> str:
        """
        Extrae caracter√≠sticas SIFT por lotes y guarda incrementalmente
        
        Args:
            image_paths: lista de rutas de im√°genes
            save_every: guardar resultados cada N im√°genes
            
        Returns:
            ruta del archivo con caracter√≠sticas
        """
        total = len(image_paths)
        output_path = "embeddings/temp_features.pkl"
        os.makedirs("embeddings", exist_ok=True)
        
        print(f"\nüñºÔ∏è  Extrayendo caracter√≠sticas SIFT en lotes")
        print(f"Total de im√°genes: {total}")
        print(f"Tama√±o de lote: {self.batch_size}")
        print(f"Guardando cada: {save_every} im√°genes")
        
        all_features = []
        processed = 0
        failed = 0
        
        for batch_start in range(0, total, self.batch_size):
            batch_end = min(batch_start + self.batch_size, total)
            batch_paths = image_paths[batch_start:batch_end]
            
            # Procesar lote
            batch_features = []
            for path in batch_paths:
                features = self._extract_single_sift(path)
                if features is not None:
                    batch_features.append((path, features))
                else:
                    failed += 1
                
                processed += 1
                
                # Mostrar progreso
                if processed % 100 == 0 or processed == total:
                    progress = processed / total * 100
                    print(f"\rProgreso: {progress:.1f}% ({processed}/{total}) - Fallos: {failed}", end='', flush=True)
            
            # Agregar al resultado total
            all_features.extend(batch_features)
            
            # Guardar incrementalmente
            if processed % save_every == 0 or processed == total:
                print(f"\nüíæ Guardando {len(all_features)} caracter√≠sticas...")
                with open(output_path, 'wb') as f:
                    pickle.dump(all_features, f)
                
                # Liberar memoria
                gc.collect()
                
                # Opcional: comprimir caracter√≠sticas antiguas
                if len(all_features) > save_every * 2:
                    # Mantener solo las √∫ltimas caracter√≠sticas en memoria
                    temp_features = all_features[-save_every:]
                    all_features = temp_features
        
        print(f"\n‚úÖ Extracci√≥n completada: {processed - failed}/{total} exitosas")
        return output_path
    
    def _extract_single_sift(self, image_path: str) -> Optional[np.ndarray]:
        """Extrae SIFT de una sola imagen con manejo de errores"""
        try:
            # Leer imagen en escala de grises
            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
            if img is None:
                return None
            
            # Redimensionar si es muy grande para ahorrar memoria
            height, width = img.shape
            max_dim = 500  # M√°ximo 500x500
            
            if height > max_dim or width > max_dim:
                scale = max_dim / max(height, width)
                new_width = int(width * scale)
                new_height = int(height * scale)
                img = cv2.resize(img, (new_width, new_height))
            
            # Extraer caracter√≠sticas
            keypoints, descriptors = self.sift.detectAndCompute(img, None)
            
            # Liberar imagen de memoria inmediatamente
            del img
            
            if descriptors is None or len(descriptors) == 0:
                # Retornar descriptor dummy si no hay caracter√≠sticas
                return np.zeros((1, 128), dtype=np.float32)
            
            # Limitar n√∫mero de descriptores para ahorrar memoria
            if len(descriptors) > 50:
                # Mantener solo los 50 m√°s fuertes
                descriptors = descriptors[:50]
            
            return descriptors.astype(np.float32)
            
        except Exception as e:
            return None
    
    def process_dataset_in_chunks(self, csv_path: str, image_column: str, 
                                 chunk_size: int = 5000) -> List[str]:
        """
        Procesa un dataset completo en chunks para manejar archivos grandes
        
        Returns:
            Lista de archivos de caracter√≠sticas generados
        """
        import pandas as pd
        
        print(f"\nüìä Procesando dataset en chunks de {chunk_size} registros")
        
        output_files = []
        chunk_num = 0
        
        # Leer CSV en chunks
        for chunk in pd.read_csv(csv_path, chunksize=chunk_size, encoding='latin1'):
            chunk_num += 1
            print(f"\nüì¶ Procesando chunk {chunk_num}")
            
            # Obtener rutas de im√°genes v√°lidas
            image_paths = []
            for path in chunk[image_column]:
                if pd.notna(path) and os.path.exists(path):
                    image_paths.append(path)
            
            if not image_paths:
                print(f"‚ö†Ô∏è  Chunk {chunk_num} no tiene im√°genes v√°lidas")
                continue
            
            # Procesar chunk
            output_file = f"embeddings/features_chunk_{chunk_num}.pkl"
            features = self.extract_sift_features_batch(image_paths, save_every=1000)
            
            # Renombrar archivo temporal
            if os.path.exists(features):
                os.rename(features, output_file)
                output_files.append(output_file)
            
            # Forzar liberaci√≥n de memoria
            del chunk
            gc.collect()
            
            print(f"‚úÖ Chunk {chunk_num} completado")
        
        return output_files
    
    @staticmethod
    def merge_feature_files(feature_files: List[str], output_path: str) -> int:
        """Combina m√∫ltiples archivos de caracter√≠sticas en uno solo"""
        print(f"\nüîÄ Combinando {len(feature_files)} archivos de caracter√≠sticas...")
        
        all_features = []
        total_features = 0
        
        for file_path in feature_files:
            if os.path.exists(file_path):
                with open(file_path, 'rb') as f:
                    features = pickle.load(f)
                    all_features.extend(features)
                    total_features += len(features)
                
                # Eliminar archivo temporal
                os.remove(file_path)
        
        # Guardar archivo combinado
        with open(output_path, 'wb') as f:
            pickle.dump(all_features, f)
        
        print(f"‚úÖ Combinados {total_features} registros en {output_path}")
        return total_features

# Funci√≥n de utilidad para uso directo
def extract_features_memory_efficient(csv_path: str, image_column: str, 
                                    output_path: str = "embeddings/features_complete.pkl"):
    """
    Funci√≥n de alto nivel para extraer caracter√≠sticas de forma eficiente
    
    Args:
        csv_path: ruta del CSV con datos
        image_column: nombre de la columna con rutas de im√°genes
        output_path: donde guardar las caracter√≠sticas
    """
    extractor = BatchImageFeatureExtractor(method='sift', batch_size=50)
    
    # Procesar en chunks
    chunk_files = extractor.process_dataset_in_chunks(
        csv_path, 
        image_column, 
        chunk_size=5000
    )
    
    # Combinar resultados
    if chunk_files:
        BatchImageFeatureExtractor.merge_feature_files(chunk_files, output_path)
        print(f"\nüéâ Proceso completado! Caracter√≠sticas guardadas en: {output_path}")
    else:
        print("\n‚ùå No se procesaron caracter√≠sticas")

if __name__ == "__main__":
    # Ejemplo de uso
    print("Extractor de caracter√≠sticas optimizado para memoria")
    print("Uso: extract_features_memory_efficient('datos.csv', 'columna_imagen')")